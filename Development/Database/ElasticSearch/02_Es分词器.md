分词器介绍
=========

分词器（Analyzer）：elasticsearch中执行的分词的主体，官方把分词器分成三个层次：

* Character Filters:针对文档的原始文本进行处理，例如将印度语的阿拉伯数字"0 12345678 9"转换成拉丁语的阿拉伯数字"0123456789"，或者去除HTML中的特殊标记符号，Character Filters可以有零或多个，安装顺序应用;
* Tokenizer:核心，将文档的原始文本按照一定规则切分为单词，Tokenizer只能有一个;
* Token Filter:对经过Tokenizer处理过后的单词进行二次加工，比如转换为小写，Token Filter也可以有多个，按顺序依次调用.

三者的调用顺序：Character Filters--->Tokenizer--->Token Filter
三者个数：analyzer = CharFilters（0个或多个） + Tokenizer(恰好一个) + TokenFilters(0个或多个)

分词使用时机

* 创建或更新文档时候，es会对相应的文档数据进行分词处理，比如你某个索引字段类型为text，那么插入一条文档时候就会对该字段进行分词处理，维护该字段文本内容的倒排索引，这种我们成为索引时分词;
* 查询时候，会对你的查询文本进行分词，比如你要查询"苹果手机"，则会分词为"苹果、手机"两个单词;

## 自定义分词器

| Setting                | Description                               |
| ---------------------- | ----------------------------------------- |
| tokenizer              | 通用的或者注册的tokenizer.                |
| filter                 | 通用的或者注册的 token filters.           |
| char_filter            | 通用的或者注册的 character filters.       |
| position_increment_gap | 距离查询时，最大允许查询的距离，默认是100 |

```sh

settings:
index :
    analysis :
        analyzer :
            myAnalyzer2 :
                type : custom
                tokenizer : myTokenizer1
                filter : [myTokenFilter1, myTokenFilter2]
                char_filter : [my_html]
                position_increment_gap: 256
        tokenizer :
            myTokenizer1 :
                type : standard
                max_token_length : 900
        filter :
            myTokenFilter1 :
                type : stop
                stopwords : [stop1, stop2, stop3, stop4]
            myTokenFilter2 :
                type : length
                min : 0
                max : 2000
        char_filter :
              my_html :
                type : html_strip
                escaped_tags : [xxx, yyy]
                read_ahead : 1024

```


## ES内置的analyzer

| analyzer           |                                               logical name                                                | description                                                                          |
| ------------------ | :-------------------------------------------------------------------------------------------------------: | :----------------------------------------------------------------------------------- |
| standard analyzer  |                                                 standard                                                  | standard tokenizer, standard filter, lower case filter, stop filter                  |
| simple analyzer    |                                                  simple                                                   | lower case tokenizer                                                                 |
| stop analyzer      |                                                   stop                                                    | lower case tokenizer, stop filter                                                    |
| keyword analyzer   |                                                  keyword                                                  | 不分词，内容整体作为一个token(not_analyzed)                                          |
| pattern analyzer   |                                                whitespace                                                 | 正则表达式分词，默认匹配\W+                                                          |
| language analyzers | [lang](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/analysis-lang-analyzer.html) | 各种语言                                                                             |
| snowball analyzer  |                                                 snowball                                                  | standard tokenizer, standard filter, lower case filter, stop filter, snowball filter |
| custom analyzer    |                                                  custom                                                   | 一个Tokenizer, 零个或多个Token Filter, 零个或多个Char Filter                         |


## ES内置的character filter列表

| character filter            |  logical name   | description                |
| --------------------------- | :-------------: | :------------------------- |
| mapping char filter         |     mapping     | 根据配置的映射关系替换字符 |
| html strip char filter      |   html_strip    | 去掉HTML元素               |
| pattern replace char filter | pattern_replace | 用正则表达式处理字符串     |

## ES内置的tokenizer列表

| tokenizer               |  logical name  | description                             |
| ----------------------- | :------------: | :-------------------------------------- |
| standard tokenizer      |    standard    |                                         |
| edge ngram tokenizer    |   edgeNGram    |                                         |
| keyword tokenizer       |    keyword     | 不分词                                  |
| letter analyzer         |     letter     | 按单词分                                |
| lowercase analyzer      |   lowercase    | letter tokenizer, lower case filter     |
| ngram analyzers         |     nGram      |                                         |
| whitespace analyzer     |   whitespace   | 以空格为分隔符拆分                      |
| pattern analyzer        |    pattern     | 定义分隔符的正则表达式                  |
| uax email url analyzer  | uax_url_email  | 不拆分url和email                        |
| path hierarchy analyzer | path_hierarchy | 处理类似`/path/to/somthing`样式的字符串 |

## ES内置的token filter列表

| token filter             |                   logical name                    | description                                                              |
| ------------------------ | :-----------------------------------------------: | :----------------------------------------------------------------------- |
| standard filter          |                     standard                      |                                                                          |
| ascii folding filter     |                   asciifolding                    |                                                                          |
| length filter            |                      length                       | 去掉太长或者太短的                                                       |
| lowercase filter         |                     lowercase                     | 转成小写                                                                 |
| ngram filter             |                       nGram                       |                                                                          |
| edge ngram filter        |                     edgeNGram                     |                                                                          |
| porter stem filter       |                    porterStem                     | 波特词干算法                                                             |
| shingle filter           |                      shingle                      | 定义分隔符的正则表达式                                                   |
| stop filter              |                       stop                        | 移除 stop words                                                          |
| word delimiter filter    |                  word_delimiter                   | 将一个单词再拆成子分词                                                   |
| stemmer token filter     |                      stemmer                      |                                                                          |
| stemmer override filter  |                 stemmer_override                  |                                                                          |
| keyword marker filter    |                  keyword_marker                   |                                                                          |
| keyword repeat filter    |                  keyword_repeat                   |                                                                          |
| kstem filter             |                       kstem                       |                                                                          |
| snowball filter          |                     snowball                      |                                                                          |
| phonetic filter          |                     phonetic                      | [插件](https://github.com/elasticsearch/elasticsearch-analysis-phonetic) |
| synonym filter           |                     synonyms                      | 处理同义词                                                               |
| compound word filter     | dictionary_decompounder, hyphenation_decompounder | 分解复合词                                                               |
| reverse filter           |                      reverse                      | 反转字符串                                                               |
| elision filter           |                      elision                      | 去掉缩略语                                                               |
| truncate filter          |                     truncate                      | 截断字符串                                                               |
| unique filter            |                      unique                       |                                                                          |
| pattern capture filter   |                  pattern_capture                  |                                                                          |
| pattern replace filte    |                  pattern_replace                  | 用正则表达式替换                                                         |
| trim filter              |                       trim                        | 去掉空格                                                                 |
| limit token count filter |                       limit                       | 限制token数量                                                            |
| hunspell filter          |                     hunspell                      | 拼写检查                                                                 |
| common grams filter      |                   common_grams                    |                                                                          |
| normalization filter     |    arabic_normalization, persian_normalization    |                                                                          |

